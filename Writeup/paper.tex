\documentclass{sig-alternate-05-2015}

\usepackage{graphicx}

\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}

\begin{document}
	
	% Copyright
	\setcopyright{acmcopyright}
	%\setcopyright{acmlicensed}
	%\setcopyright{rightsretained}
	%\setcopyright{usgov}
	%\setcopyright{usgovmixed}
	%\setcopyright{cagov}
	%\setcopyright{cagovmixed}
	
	
	% DOI
	\doi{n/a}
	
	% ISBN
	\isbn{n/a}

\date{\today}
\title{\ttlfnt{Comparing the performance of websites and their content distribution networks}}
\author{
	\begin{tabular}{c}
		% 1st. author
		Jacques Heunis \\
		\affaddr{University of Cape Town}\\
		\email{hnsjac003@myuct.ac.za}
	\end{tabular}%
	\begin{tabular}{c}
		% 2nd. author
		Brian McGeorge \\
		\affaddr{University of Cape Town}\\
		\email{mcgbri004@myuct.ac.za}
	\end{tabular} 
}

\maketitle

\begin{abstract}
Modern internet infrastructure consists of deep hierarchies of caches and Content Delivery Networks in order to serve the ever-increasing demand for high-bandwidth multimedia content. In this report we present a tool for measuring the impact of Content Delivery Networks on the performance of streaming video and audio content over the internet. We analyse the performance impact of these networks from the point of view both of the home user and large-scale institutions. We find that the majority of unpopular videos are not cached within South Africa. The video has to be obtained from a Google cache in Europe. We find that after viewing the unpopular video it is cached on both on the ISP's server and Google's South African cache. Subsequent views from the same ISP come from the ISP cache and views from another ISP come from Google's South African cache instead of from Europe. On the institutional network we find that even though, on the second execution, an unpopular YouTube video is obtained from the institution's cache, the throughput is always slower than the third execution.
\end{abstract}

\section{Introduction}\label{sec:intro}
In the modern internet, 65\% of web traffic is handled by just 10 organisations \cite{Gehlen2012}. These organisations have to serve tremendous amounts of data to a large user base across the globe. Vast Content Delivery Networks (CDN) and data centres are therefore required to serve up all this content. Approximately 90\% of Google's traffic and over 50\% of Level3 and Limelight's traffic is from video content alone \cite{Gehlen2012}. Labovitz \textit{et al.} \cite{Labovitz:2010:IIT:2043164.1851194} suggests that videos account for $25-40\%$ of all HTTP traffic. 

Of the video streaming services on the internet, YouTube is most popular with over a billion users \cite{youtubeStats}. Everyday, YouTube generates billions of views with people watching hundreds of million of hours of content \cite{youtubeStats}. With new content constantly getting generated around the world, caching strategies are required so that this content can be served up at a high throughput from as close as possible to the user. Internet Service Providers (ISPs) play a critical role in this through their peering policies and by providing their own CDNs for various content \cite{Labovitz:2010:IIT:2043164.1851194}.

Our aim in this paper is to investigate the performance of the CDN's that serve this content. We will examine different streaming services across a variety of South African ISPs to examine how traffic is routed and where it comes from. In addition, we will also examine the caching behaviour of YouTube, which has local caches in South Africa. We have developed a tool which automates many of the aforementioned measurements. It can take a list of URLs to content on a streaming service and record various performance metrics regarding how the content is delivered to the end-user.

Section \ref{sec:related} will examine related work in measuring the performance of streaming services and studies which investigate the behaviour of CDNs for multimedia streaming services. Section \ref{sec:method} describes the tool that we developed and details the steps we followed in using it to capture our results. Section \ref{sec:results} presents the results and discussions thereof.

\section{Related Work}\label{sec:related}
% Relevant work that has been done
There have been a number of studies on how the performance of video streaming affects the Quality of Experience (QoE) for the user. Casas \textit{et al.} \cite{6975242} took an ISP traffic view by analysing 1 month's worth of YouTube packet flows from a large European ISP. The study challenges Google's Video Quality Report\footnote{http://www.google.com/get/videoqualityreport/} as it only considers ISP and access link bottlenecks as the root cause of bad user experience \cite{6975242}. Casas \textit{et al.} \cite{6975242} presents how poor content server selection by Google resulted in a large scale reduction in QoE for many YouTube users. Fiadino \textit{et al.} \cite{6932930} developed a tool for ISPs to detect unexpected cache-selection events and changes in the traffic delivered by CDNs. Anomalous CDN behaviour has been shown to have a large impact for ISPs carrying the traffic and end-users \cite{6932930, Plissonneau:2012:LVH:2155555.2155588}. Zhu \textit{et al.} \cite{6233056} developed LatLong, a tool for CDNs to diagnose large latency increases on the network through passive measurement of traffic and routing. Results from analysing a one month's data from Google's CDN show that more than 40\% of latency increases coincide with inter-domain routing changes and more than one-third involve a shift in traffic to different servers. Juluri \textit{et al.} \cite{6038496} developed Pytomo, a tool to measure playback quality on an end-host. The tool captures various metrics such as delay towards the server, download rates and buffering duration \cite{6038496}. These can then be used to measure QoE.

In 2007 Gill \textit{et al.} \cite{Gill:2007:YTC:1298306.1298310} studied the usage patterns, file properties, popularity and transfer behaviour of YouTube. Its findings found that caching could be used effectively to scale Web 2.0 applications such as YouTube \cite{Gill:2007:YTC:1298306.1298310}. In 2011, Torres \textit{et al.} \cite{5961681} analysed a week-long dataset collected from the edge of five networks. The study determined the location of YouTube servers and revealed that round-trip time (RTT) is not the only factor determining video server selection \cite{5961681}. Other factors that were found to affect server selection were load-balancing, the DNS server used and popularity of the accessed video \cite{5961681}.

Labovitz \textit{et al.} \cite{Labovitz:2010:IIT:2043164.1851194} analysed 2 years worth of internet traffic through 110 different cable operators, international transit backbones, regional networks and content providers. The study found that there was a significant rise in video traffic over the 2 year period \cite{Labovitz:2010:IIT:2043164.1851194}. It also found that the majority of inter-domain traffic is between large content providers, CDNs and consumer networks \cite{Labovitz:2010:IIT:2043164.1851194}.
\\\\\\
\section{Method}\label{sec:method}
\subsection{Measurement and tools}
To measure the performance of CDNs, we have created a tool to automatically capture various quantitative and qualitative performance metrics when given a list of URLs to online video or audio content. In order to compare the performance of CDNs to that of the websites that present the content, we measure 2 primary quantitative metrics: ping or RTT and the number of hops required to reach the server. We record measurements using the resolved IP of the input URL (later referred to as ``website''), the IP that the browser is directed to obtain the content from (later referred to as ``CDN'') and the IP of the server that actually served up the multimedia content (later referred to as ``content''). We also measure the throughput from the multimedia CDN to end-host. To gain further insight into the potential causes for variances in performance, we also take note of the location and hosting organisation for each of the aforementioned IPs. Lastly, we record the input URL in our output so that each result can be easily identified and aggregated. The source code for our tool is publicly available on GitHub\footnote{CDNPerf -  https://github.com/jacquesh/cdnperf}.

Our tool is written using Python 3, but makes use of a number of third-party tools and services. It makes use of the youtube-dl\footnote{youtube-dl: available at https://rg3.github.io/youtube-dl/} tool to download multimedia content (which is used to measure bandwidth, as detailed below). We examined packet traces and found that youtube-dl exhibited the same behaviour as viewing the content through the browser. The tool uses tcpdump to passively capture network traffic (on Windows we use WinDump which is a Windows port of tcpdump, but for the purposes of this report we will refer to tcpdump). The ipinfo.io service\footnote{ipinfo: available at http://ipinfo.io/} is used to gather information about the organisation that is hosting the website and CDN IP addresses. It is also used to determine the approximate location of each IP. 

For each input URL we run youtube-dl and ask that it simply return the URL that directs us to the content. Both URLs are resolved to IP addresses and stored for later measurement. We then run tcpdump, specifying that it will terminate after 3000 packets have been received. This is just to gather enough packets that we can be fairly sure that our results are not contaminated by another process using the network at an unfortunate time. We then run youtube-dl again, but this time we let it actually download the media file, allowing tcpdump to capture the required 3000 packets and then terminate. At this point we can kill the youtube-dl process and just consider its output. First, we look at the last line of output from youtube-dl in order to get the throughput that the content was transmitted at. This does mean that we rely on youtube-dl's measurements being accurate, but previous attempts to measure throughput ourselves proved inaccurate as youtube-dl does some work before downloading (for example to get the CDN URL from the webpage).

Next we go through every packet that tcpdump captured, we discard every packet where the source IP is the local machine's IP. Of these filtered packets, we find the IP that sent us the most packets. We consider that to be the IP that the content actually came from, as long as more than one half of the filtered packets came from that IP. In the event that no single IP was the source of more than a half of the filtered packets, we retry up to three times. This retry behaviour gives our test greater stability in the face of existing network traffic. Now we have an IP for the website, the CDN, and the actual content, and we run ping and traceroute, as well as querying ipinfo.io for all three of these.

In order to reduce the chance of our results being affected by other applications that might be using the network, we first ping Google's 8.8.8.8 DNS server and listen to the existing traffic on the network when the tool first starts up. If we get a round-trip time of over 150ms or there is more than 100KB/s of incoming traffic or more than 30KB/s of outgoing traffic, we prompt the user to reduce network traffic and do not proceed with the test. This gives our results greater accuracy and repeatability as it prevents us from running the experiment when there is already high-network usage, which would skew the data.

In order to further improve the accuracy of our results, we run each test 3 times for each input URL. This prevents a short-running network-intensive process from executing only during one test and leading us to believe that one particular website or piece of content is received significantly more slowly than the others.

\subsection{Sources of multimedia content}
As mentioned in Section \ref{sec:intro}, YouTube is one of the most popular video streaming services on the internet. Since this is not a study of YouTube specifically, we need to gather data on a number of other multimedia streaming services as well. We decided to also gather data about the popular music sharing service Soundcloud, as well as the popular live video streaming platform Twitch.tv. Note that our analysis is restricted to on-demand multimedia streaming and we do not consider the performance of live streaming services on YouTube or Twitch.tv. Note also that we did not include Netflix in our evaluation. While Netflix does constitute a large portion of internet video streaming traffic in the US, it is far less popular in South Africa and so its performance is of less importance here.

Of particular interest is the effect of caching on the performance of YouTube videos. It is widely known that there are Google caches in South Africa and that local ISPs have their own caches. For this reason we specifically included performance evaluations for YouTube videos that are unpopular or have very few views. To find such videos, we manually looked through the ``unknownvideos'' section of Reddit\footnote{``/r/unknownvideos - Watch something new'', available at https://www.reddit.com/r/unknownvideos}.

\subsection{Measurement Location}
We measure two different scenarios: home networks and institutional networks. In the case of home networks, all our measurements were done from the same location (in order to ensure consistency in our results). Home network measurements were taken while connected to five different ISPs (namely Afrihost, Axxess, Cybersmart, Telkom, and WebAfrica) for comparison.

In the case of institutional networks, all our measurements were done while connected to the University of Cape Town (UCT) network. This test was added in order to compare the results from a home network to what one might get on a network with significantly higher bandwidth.

\section{Results}\label{sec:results}
\subsection{Home network}
We ran our tests on a single home network, gathering measurements for 5 different ISPs (namely Afrihost, Axxess, Cybersmart, Telkom, and WebAfrica) using the same four media URLs\footnote{Unpopular YouTube video: \\ https://www.youtube.com/watch?v=1-xX7hPxMio}\footnote{Popular YouTube video: \\ https://www.youtube.com/watch?v=9bZkp7q19f0}\footnote{Soundcloud audio: \\ https://soundcloud.com/nocopyrightsounds/lensko-cetus-ncs-release}\footnote{Twitch.tv video: \\https://www.twitch.tv/dota2ti/v/83400929} on a 2 megabits per second (Mbps) ADSL line from Telkom. The Telkom ISP account is uncapped while the rest of the ISP accounts are capped at 1 gigabyte.

\subsubsection{Results across ISPs}
Table \ref{table:avgPingAcrossISP}, \ref{table:avgHopsAcrossISP} and \ref{avgThroughputAcrossISP} show the RTT, number of hops and throughput averaged across ISPs.
\begin{table}
	\caption{Averaged RTT across ISPs (ms)}
	\label{table:avgPingAcrossISP}
	\makebox[\linewidth]{
	\small
	\begin{tabular}{|l|l|c|c|c|} \hline
	Run & Source & Website & CDN & Content \\ \hline
	1 & Unpopular YouTube & 39  & 35  & 56  \\ \hline
	  & Popular YouTube   & 60  & 14  & 14  \\ \hline
	  & Soundcloud        & 161 & 162 & 184 \\ \hline
	  & Twitch.tv         & 198 & 159 & 172 \\ \hline
	2 & Unpopular YouTube & 45  & 14  & 17  \\ \hline
	  & Popular YouTube   & 63  & 14  & 31  \\ \hline
	  & Soundcloud        & 161 & 160 & 165 \\ \hline
	  & Twitch.tv         & 193 & 159 & 181 \\ \hline
	3 & Unpopular YouTube & 31  & 17  & 14  \\ \hline
	  & Popular YouTube   & 31  & 23  & 18  \\ \hline
	  & Soundcloud        & 179 & 171 & 160 \\ \hline
	  & Twitch.tv         & 184 & 159 & 161 \\ \hline
	\end{tabular}}
\end{table}
\begin{table}
	\caption{Averaged number of hops across ISPs}
	\label{table:avgHopsAcrossISP}
	\makebox[\linewidth]{
	\small
	\begin{tabular}{|l|l|c|c|c|} \hline
	Run & Source & Website & CDN & Content \\ \hline
	1 & Unpopular YouTube & 10 & 7  & 10 \\ \hline
	  & Popular YouTube   & 10 & 8  & 8  \\ \hline
	  & Soundcloud        & 13 & 18 & 18 \\ \hline
	  & Twitch.tv         & 14 & 14 & 15 \\ \hline
	2 & Unpopular YouTube & 10 & 7  & 7  \\ \hline
	  & Popular YouTube   & 10 & 8  & 8  \\ \hline
	  & Soundcloud        & 13 & 18 & 18 \\ \hline
	  & Twitch.tv         & 13 & 15 & 12 \\ \hline
	3 & Unpopular YouTube & 10 & 7  & 7  \\ \hline
	  & Popular YouTube   & 10 & 8  & 8  \\ \hline
	  & Soundcloud        & 13 & 18 & 18 \\ \hline
	  & Twitch.tv         & 13 & 15 & 13 \\ \hline
	\end{tabular}}
\end{table}

\begin{table}
	\caption{Averaged throughput across ISPs (KB/s)}
	\label{avgThroughputAcrossISP}
	\makebox[\linewidth]{
	\small
	\begin{tabular}{|l|l|c|c|c|} \hline
	Run & Source & Content \\ \hline
	1 & Unpopular YouTube & 197.8 \\ \hline
	  & Popular YouTube   & 202.6 \\ \hline
	  & Soundcloud        & 197.9 \\ \hline
	  & Twitch.tv         & 201.2 \\ \hline
	2 & Unpopular YouTube & 201.9 \\ \hline
	  & Popular YouTube   & 197.8 \\ \hline
	  & Soundcloud        & 201.3 \\ \hline
	  & Twitch.tv         & 203.1 \\ \hline
	3 & Unpopular YouTube & 202.8 \\ \hline
	  & Popular YouTube   & 200.4 \\ \hline
	  & Soundcloud        & 203.6 \\ \hline
	  & Twitch.tv         & 204.9 \\ \hline
	\end{tabular}}
\end{table}
It is immediately clear that the throughput does not differ much across websites or runs, although this is not unexpected given the relatively low bandwidth of the home connection. Another trend which is immediately clear is the significant difference between RTT and number of hops to YouTube and Soundcloud/Twitch.tv. This is simply because YouTube content is cached within South Africa while Twitch.tv and Soundcloud do not, most likely because they are not large enough to be able to afford setting up expensive caches all over the world.

One thing that is less obvious is the difference of the first run to the other two for the unpopular YouTube video. In runs 2 and 3 the actual content came from the same organisation that hosts the CDN, but in run 1 we see significant increase in latency and note that the content comes from a different organisation. The cause of this is clear if we look at Table \ref{table:LatencyToUnpopularYouTubeVideo} and \ref{table:OrgHostingUnpopularYouTubeVideo} which shows the RTT to the content IP and the organisation that manages it.

\begin{table}
	\caption{RTT to unpopular YouTube video (ms)}
	\label{table:LatencyToUnpopularYouTubeVideo}
	\makebox[\linewidth]{
	\small
	\begin{tabular}{|l|c|c|c|c|c|} \hline
	Run & Afrihost & Axxess & Cybersmart & Telkom & WebAfrica \\ \hline
	1 & 11 & 11  & 34  & 197 & 27 \\ \hline
	2 & 11 & 11  & 12  & 9   & 42 \\ \hline
	3 & 11 & 11  & 12  & 9   & 26 \\ \hline
	\end{tabular}}
\end{table}

\begin{table*}
	\centering
	\caption{Organisation managing IP to unpopular YouTube video}
	\label{table:OrgHostingUnpopularYouTubeVideo}
	\makebox[\linewidth]{
		\small
		\begin{tabular}{|l|c|c|c|c|c|} \hline
			Run & Afrihost & Axxess & Cybersmart & Telkom & WebAfrica \\ \hline
			1 & Dimension Data - Optinet & Dimension Data - Optinet & Google Inc. & Google Inc. & Google Inc. \\ \hline
			2 & Dimension Data - Optinet & Dimension Data - Optinet & Dimension Data - Optinet & Telkom SA Ltd. & Internet Solutions \\ \hline
			3 & Dimension Data - Optinet & Dimension Data - Optinet & Dimension Data - Optinet & Telkom SA Ltd. & Internet Solutions \\ \hline
		\end{tabular}}
	\end{table*}

The tests did 3 runs on Telkom first, followed by WebAfrica, Cybersmart, Axxess and finally Afrihost to the same unpopular video. On the first run (which just happens to be via Telkom) the video is not cached anywhere in South Africa, as a result it has to come from a Google cache in Europe. However, this causes it to get cached so the following runs get served from Telkom's own cache. What we see from WebAfrica in the subsequent run is that the video does not come from Europe or Telkom's cache. It instead comes from a Google cache in South Africa. This means that when we first retrieved the video, it was cached not only to Telkom's servers, but also to Google's South African servers. Both WebAfrica and Cybersmart first serve the video up using Google's South African cache then serve it up using their respective ISP cache. Since Afrihost and Axxess use the same ISP cache as Cybersmart, they are served up directly with the ISP cache in their subsequent runs. 

A follow-up test was done to check if Telkom also saw this behaviour that they will fetch a video from Google's South African cache if a user from another South African ISP has watched that video but a Telkom user has not. This was found to be the case

One more thing to notice is that in most cases, the RTT to the website is higher than to the CDN or the content. This is a useful result as it indicates that content delivery networks are providing an optimized means of delivering high-bandwidth content to users.

\subsubsection{Results across runs}
To investigate the differences between ISPs, we present the offset from the median RTT and throughput averaged across the 3 runs. Table \ref{table:OffsetFromMedianPingToWebsite}, \ref{table:OffsetFromMedianPingToCDN} and \ref{table:OffsetFromMedianPingToContent} contain the offset from median RTT to website, CDN and content IP from each ISP respectively. Table \ref{table:OffsetFromMedianThroughput} contains the offset median throughput for each ISP.

\begin{table}
	\caption{Offset from median RTT to website (ms, lower is better)}
	\label{table:OffsetFromMedianPingToWebsite}
	\makebox[70mm]{
	\small
	\begin{tabular}{|l|c|c|c|c|c|} \hline
	 & \small{Afrihost} & \small{Axxess} & \small{Cybersmart} & \small{Telkom} & \small{WebAfrica} \\ \hline
	\small{Unpopular YT} & 9  & 23  & 2  & -1  & -3 \\ \hline
	Popular YT           & 11 & 88  & 1  & -1  & -6 \\ \hline
	Soundcloud           & 10 & 0   & -5 & -3  & 53 \\ \hline
	Twitch.tv            & 26 & 6   & 14 & -9  & 41 \\ \hline
	Total                & 56 & 117 & 12 & -14 & 85 \\ \hline
	\end{tabular}}
\end{table}

\begin{table}
	\caption{Offset from median RTT to CDN (ms, lower is better)}
	\label{table:OffsetFromMedianPingToCDN}
	\makebox[70mm]{
	\small
	\begin{tabular}{|l|c|c|c|c|c|} \hline
	 & \small{Afrihost} & \small{Axxess} & \small{Cybersmart} & \small{Telkom} & \small{WebAfrica} \\ \hline
	\small{Unpopular YT} & 35 & -1 & 0  & -2 & 21 \\ \hline
	Popular YT           & 0  & 0  & 0  & -2 & 28 \\ \hline
	Soundcloud           & 14 & 2  & -6 & -1 & 25 \\ \hline
	Twitch.tv            & -5 & -4 & 1  & 1  & 12 \\ \hline
	Total                & 44 & -4 & -4 & -3 & 86 \\ \hline
	\end{tabular}}
\end{table}

\begin{table}
	\caption{Offset from median RTT to content (ms, lower is better)}
	\label{table:OffsetFromMedianPingToContent}
	\makebox[70mm]{
	\small
	\begin{tabular}{|l|c|c|c|c|c|} \hline
	 & \small{Afrihost} & \small{Axxess} & \small{Cybersmart} & \small{Telkom} & \small{WebAfrica} \\ \hline
	\small{Unpopular YT} & -5  & -5 & 3  & 56 & 15 \\ \hline
	Popular YT           & 0   & 7  & 0  & -2 & 45 \\ \hline
	Soundcloud           & -8  & -1 & 2  & -1 & 52 \\ \hline
	Twitch.tv            & 0   & 29 & -3 & 15 & 16 \\ \hline
	Total                & -14 & 29 & 2  & 68 & 129 \\ \hline
	\end{tabular}}
\end{table}

\begin{table}
	\caption{Offset from median throughput (KB/s, higher is better)}
	\label{table:OffsetFromMedianThroughput}
	\makebox[95mm]{
	\small
	\begin{tabular}{|l|c|c|c|c|c|} \hline
	 & \small{Afrihost} & \small{Axxess} & \small{Cybersmart} & \small{Telkom} & \small{WebAfrica} \\ \hline
	\small{Unpopular YT} & 1.5  & -0.5  & 2.1   & -3.9 & -2.9 \\ \hline
	Popular YT           & -0.8 & -8.5  & -3.1  & 0.4  & -1.2 \\ \hline
	Soundcloud           & 0.1  & -5.1  & -1.3  & 2.5  & -5.4 \\ \hline
	Twitch.tv            & -0.4 & 0.3   & -6.8  & 0.7  & -0.6 \\ \hline
	Total                & 0.4  & -13.8 & -9.2  & -0.3 & -10.2\\ \hline
	\end{tabular}}
\end{table}
While each ISP clearly performs differently, the only significant trend is that WebAfrica consistently gave a higher RTT than other ISPs. The only exception to this is the RTT to the YouTube website where WebAfrica performed slightly better than the median.

The differences in throughput are not significant and while there are some significant differences in latency, they are not consistent and so are likely due to external factors on the network.

\subsubsection{Content server owners}
Although not shown in the tables, it is worth noting the results of the WHOIS lookup for the IPs that served the content. In the case of YouTube, which server provides the content depends on which networks make use of which local caches as well as whether or not the requested content is available in the cache. This is expected as YouTube is a very popular source of content and ISPs setup complex caching systems to reduce bandwidth. For Soundcloud and Twitch.tv however, this is not the case. All Soundcloud content gets delivered from a IP address that is registered to Amazon, suggesting that Soundcloud uses Amazon Web Services for all of its content storage and distribution. The content servers for Twitch.tv are not as consistent. While Afrihost, Telkom and WebAfrica all receive Twitch content from an IP registered to ``MCI Communications Services'' (the same company that is the registered owner of the CDN that hosts the content), Axxess and Cybersmart do not. Axxess routes to a CDN owned by MCI, but the actual content comes from a server owned by ``Akamai International''. Cybersmart does not even receive Twitch content from the same server each time, content arrives from servers that are owned by a variety of other companies.

\subsection{UCT Network}
For a comparison, we also ran two tests on the UCT network. The test was run from a laptop connected to the UCT network via Ethernet running at 100 Mbps.

Note that some of the throughput data points are missing, this is because we measure throughput as youtube-dl downloads and in those cases the download finished before we could get a good measure.
\subsubsection{Test case 1}
The first test case used the following media URLs:
\begin{itemize}
	\item Unpopular YouTube video: \\ 
	https://www.youtube.com/watch?v=yRfwRsJrMzI
	\item Popular YouTube video: \\
	https://www.youtube.com/watch?v=9bZkp7q19f0
	\item Soundcloud audio: \\ https://soundcloud.com/nocopyrightsounds/lensko-cetus-ncs-release
	\item Twitch.tv video: \\ https://www.twitch.tv/dota2ti/v/83400929
\end{itemize}
Table \ref{table:UCTPingTests-TestCase1}, \ref{table:UCTHopCount-TestCase1} and \ref{table:UCTThroughput-TestCase1} contain the RTT, number of hops and throughput for each run of test case 1.

\begin{table}
    \centering
    \caption{RTT using UCT network (ms) - Test 1}
    \label{table:UCTPingTests-TestCase1}
    \makebox[\linewidth]{
    \small
    \begin{tabular}{|l|l|c|c|c|} \hline
    Run & Source & Website & CDN & Content \\ \hline
    1 & Unpopular YouTube & 19  & 18  & 184 \\ \hline
      & Popular YouTube   & 18  & 18  & 18  \\ \hline
      & Twitch.tv         & 148 & 148 & 148 \\ \hline
      & Soundcloud        & 148 & 149 & 149 \\ \hline
    2 & Unpopular YouTube & 19  & 18  & 18  \\ \hline
      & Popular YouTube   & 19  & 18  & 19  \\ \hline
      & Twitch.tv         & 148 & 150 & 153 \\ \hline
      & Soundcloud        & 148 & 148 & 192 \\ \hline
    3 & Unpopular YouTube & 19  & 19  & 18  \\ \hline
      & Popular YouTube   & 19  & 18  & 18  \\ \hline
      & Twitch.tv         & 148 & 152 & 148 \\ \hline
      & Soundcloud        & 148 & 149 & 150 \\ \hline
    \end{tabular}}
\end{table}

\begin{table}
	\centering
	\caption{Hop count using UCT network - Test 1}
	\label{table:UCTHopCount-TestCase1}
	\makebox[\linewidth]{
	\small
    \begin{tabular}{|l|l|c|c|c|} \hline
    Run & Source & Website & CDN & Content \\ \hline
    1 & Unpopular YouTube & 9 & 9 & 16 \\ \hline
      & Popular YouTube   & 9 & 9 & 9 \\ \hline
      & Twitch.tv         & 13 & 11 & 11 \\ \hline
      & Soundcloud        & 11 & 18 & 18 \\ \hline
    2 & Unpopular YouTube & 10 & 9 & 9 \\ \hline
      & Popular YouTube   & 10 & 9 & 9 \\ \hline
      & Twitch.tv         & 12 & 13 & 13 \\ \hline
      & Soundcloud        & 11 & 17 & 17 \\ \hline
    3 & Unpopular YouTube & 10 & 9 & 9 \\ \hline
      & Popular YouTube   & 9 & 9 & 9 \\ \hline
      & Twitch.tv         & 13 & 13 & 13 \\ \hline
      & Soundcloud        & 11 & 18 & 18 \\ \hline
    \end{tabular}}
\end{table}

\begin{table}
	\centering
	\caption{Throughput using UCT network (KB/s) - Test 1}
	\label{table:UCTThroughput-TestCase1}
	\makebox[\linewidth]{
	\small
    \begin{tabular}{|l|l|c|c|c|} \hline
    Run & Source & Content (KB/s)    \\ \hline
    1 & Unpopular YouTube & 521.5  \\ \hline
      & Popular YouTube   & 9809.9 \\ \hline
      & Twitch.tv         & 11366.4    \\ \hline
      & Soundcloud        & -    \\ \hline
    2 & Unpopular YouTube & 6481.9  \\ \hline
      & Popular YouTube   & 9840.6  \\ \hline
      & Twitch.tv         & 3440.6    \\ \hline
      & Soundcloud        & -    \\ \hline
    3 & Unpopular YouTube & 8069.1  \\ \hline
      & Popular YouTube   & 9881.6 \\ \hline
      & Twitch.tv         & 11335.7    \\ \hline
      & Soundcloud        & -    \\ \hline
    \end{tabular}}
\end{table}

\subsubsection{Test case 2}
The second test case used the following media URLs:
\begin{itemize}
	\item Unpopular YouTube video: \\ 
	https://www.youtube.com/watch?v=tSzaFuCxjs0
	\item Popular YouTube video: \\
	https://www.youtube.com/watch?v=9bZkp7q19f0
	\item Soundcloud audio: \\ https://soundcloud.com/nocopyrightsounds/lensko-cetus-ncs-release
	\item Twitch.tv video: \\ https://www.twitch.tv/dota2ti/v/83400929
\end{itemize}
Table \ref{table:UCTPing-TestCase2}, \ref{table:UCTHopCount-TestCase2} and \ref{table:UCTThroughput-TestCase2} contain the RTT, number of hops and throughput for run of test case 2.

\begin{table}
	\centering
	\caption{RTT using UCT network (ms) - Test 2}
	\label{table:UCTPing-TestCase2}
	\makebox[\linewidth]{
	\small
    \begin{tabular}{|l|l|c|c|c|} \hline
    Run & Source & Website & CDN & Content \\ \hline
    1 & Unpopular YouTube & 19 & 3 & 182  \\ \hline
      & Popular YouTube   & 19 & 18 & 18  \\ \hline
      & Twitch.tv         & 148 & 150 & 148  \\ \hline
      & Soundcloud        & 148 & 148 & 148  \\ \hline
    2 & Unpopular YouTube & 19 & 3 & 3  \\ \hline
      & Popular YouTube   & 19 & 18 & 18  \\ \hline
      & Twitch.tv         & 148 & 150 & 150  \\ \hline
      & Soundcloud        & 148 & 148 & 148  \\ \hline
    3 & Unpopular YouTube & 19 & 3 & 3  \\ \hline
      & Popular YouTube   & 20 & 18 & 18 \\ \hline
      & Twitch.tv         & 148 & 148 & 148  \\ \hline
      & Soundcloud        & 148 & 149 & 148  \\ \hline
    \end{tabular}}	
\end{table}

\begin{table}
	\centering
	\caption{Hop count using UCT network - Test 2}
	\label{table:UCTHopCount-TestCase2}
	\makebox[\linewidth]{
	\small
    \begin{tabular}{|l|l|c|c|c|} \hline
    Run & Source & Website & CDN & Content \\ \hline
    1 & Unpopular YouTube & 9 & 7 & 16  \\ \hline
      & Popular YouTube   & 10 & 9 & 9  \\ \hline
      & Twitch.tv         & 12 & 13 & 13  \\ \hline
      & Soundcloud        & 11 & 17 & 17  \\ \hline
    2 & Unpopular YouTube & 10 & 7 & 7  \\ \hline
      & Popular YouTube   & 9 & 9 & 9  \\ \hline
      & Twitch.tv         & 13 & 13 & 13  \\ \hline
      & Soundcloud        & 11 & 16 & 16  \\ \hline
    3 & Unpopular YouTube & 10 & 7 & 7  \\ \hline
      & Popular YouTube   & 9 & 9 & 9  \\ \hline
      & Twitch.tv         & 12 & 12 & 12  \\ \hline
      & Soundcloud        & 11 & 18 & 18  \\ \hline
    \end{tabular}}
\end{table}

\begin{table}
	\centering
	\caption{Throughput using UCT network (KB/s) - Test 2}
	\label{table:UCTThroughput-TestCase2}
	\makebox[\linewidth]{
	\small
    \begin{tabular}{|l|l|c|c|c|} \hline
    Run & Source & Content    \\ \hline
    1 & Unpopular YouTube & 1525.8 \\ \hline
      & Popular YouTube   & 9799.7 \\ \hline
      & Twitch.tv         & 11427.8 \\ \hline
      & Soundcloud        & - \\ \hline
    2 & Unpopular YouTube & 4331.5 \\ \hline
      & Popular YouTube   & 9820.2 \\ \hline
      & Twitch.tv         & 11397.1 \\ \hline
      & Soundcloud        & - \\ \hline
    3 & Unpopular YouTube & 11274.2 \\ \hline
      & Popular YouTube   & 9175.0 \\ \hline
      & Twitch.tv         & 11325.4 \\ \hline
      & Soundcloud        & 1689.6 \\ \hline
    \end{tabular}}
\end{table}

\subsubsection{Discussions}
Being a significantly higher-bandwidth connection than what is available to a home user, this test over UCT's network makes the effects of caching and performance of CDNs more pronounced. Note the difference in throughput for the unpopular YouTube video on the first run to the second and third runs. On run 1 the video is not cached (as is evident by the higher RTT) and is delivered at a bitrate far below what the connection supports. On the run 2 the video has been cached and is delivered with a significantly higher throughput. What is interesting to note however, is that on run 3 the throughput is again significantly higher than run 2, even though the RTT to the server that delivers it has not changed. This suggests that the cache considers it to be ``hotter'' because of the additional request for it, and serves it at a higher throughput. Another possibility is that while video is available at the server, the caching process has not fully completed, leading to slower throughput on run 2. This behaviour was repeated in subsequent tests on other unpopular YouTube videos. For brevity we have not included the results of the additional tests in this paper.

Another trend to take note of is that the throughput recorded for Twitch.tv is actually significantly higher than that of YouTube, even for very popular videos. This is surprising since the Twitch.tv content is not being delivered from a server residing on the African continent, while the popular YouTube content is being delivered from caches within Cape Town.

\section{Future Work}\label{sec:futurework}
Our experiments have focussed on streaming of internet video and audio on-demand, from websites including YouTube, Twitch.tv and Soundcloud. In future work we would like to extend this to streaming of live video and audio content such as from YouTube Live or Twitch.tv's live streams. Another extension would be to consider the differences in performance of streamed video content to video adverts, which are prevalent both on YouTube and Twitch.tv.

Another direction that would be useful in future work is to run a more distributed version of our experiments which include running the test from various locations around the world. This would give a better understanding of how our results generalize to the internet as a whole, rather than just the connectivity in South Africa.

\section{Conclusions}
We have developed a tool which can take a list of URLs to a range of multimedia services. It captures a wide range of quantitative metrics such as: RTT, number of hops required to reach the IP, approximate location of IP and organisation hosting IP. Each of the aforementioned metrics are recorded for the resolved IP of the input URL, the IP that the browser is directed to obtain the content from and the IP of the server that actually served up the multimedia content. We also measure the throughput from the multimedia CDN to end-host.

We executed our tool on various multimedia services using a number of ISPs on a home network as well as an institutional network. Our results gained insights on the caching behaviour of YouTube with respect to videos with very few views. We found that the majority of unpopular videos are not cached within South Africa. The video has to be obtained from a Google cache in Europe. We found that after viewing the unpopular video it would be cached on both on the ISP's server and Google's South African cache. Subsequent views from the same ISP would come from the ISP cache and views from another ISP would come from Google's South African cache instead of from Europe.

On the institutional network we found that even though, on run 2, the unpopular YouTube video was obtained from the institution's cache, it would always be slower than run 3. It was reasoned that this could be because the cache considers it to be ``hotter'' because of the additional request for it, and serves it at a higher throughput. Another possibility is that while the video is available at the server, the caching process has not fully completed, leading to slower throughput on run 2.

\small
\bibliographystyle{abbrv}
\bibliography{paper}


\end{document}
